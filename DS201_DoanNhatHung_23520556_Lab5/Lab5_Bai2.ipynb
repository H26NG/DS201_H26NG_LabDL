{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTh8ZiDNzvMR",
        "outputId": "212d7c46-2cff-4bd9-85aa-133feb94b1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cwd: /content/PhoNER_COVID19\n",
            "word files: ['data/word/dev_word.json', 'data/word/train_word.json', 'data/word/dev_word.conll', 'data/word/test_word.json', 'data/word/test_word.conll', 'data/word/train_word.conll']\n"
          ]
        }
      ],
      "source": [
        "import os, glob\n",
        "os.system(\"git clone https://github.com/VinAIResearch/PhoNER_COVID19.git\")\n",
        "os.chdir(\"PhoNER_COVID19\")\n",
        "\n",
        "print(\"cwd:\", os.getcwd())\n",
        "print(\"word files:\", glob.glob(\"data/word/*\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_conll(path):\n",
        "    sents_tokens, sents_tags = [], []\n",
        "    tokens, tags = [], []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    sents_tokens.append(tokens)\n",
        "                    sents_tags.append(tags)\n",
        "                    tokens, tags = [], []\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            # thường là: token tag (2 cột)\n",
        "            token = parts[0]\n",
        "            tag = parts[-1]\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "    if tokens:\n",
        "        sents_tokens.append(tokens)\n",
        "        sents_tags.append(tags)\n",
        "    return sents_tokens, sents_tags\n",
        "\n",
        "train_tokens, train_tags = read_conll(\"data/word/train_word.conll\")\n",
        "dev_tokens, dev_tags     = read_conll(\"data/word/dev_word.conll\")\n",
        "test_tokens, test_tags   = read_conll(\"data/word/test_word.conll\")\n",
        "\n",
        "len(train_tokens), len(dev_tokens), len(test_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NMqf3Ay0XNZ",
        "outputId": "3aa45176-0307-42f9-823f-eea91925a7e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5027, 2000, 3000)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "print(train_tokens[i][:30])\n",
        "print(train_tags[i][:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRapRfNd0b2n",
        "outputId": "628738a2-3717-4f36-8609-e25c3f7f63d4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Đồng_thời', ',', 'bệnh_viện', 'tiếp_tục', 'thực_hiện', 'các', 'biện_pháp', 'phòng_chống', 'dịch_bệnh', 'COVID', '-', '19', 'theo', 'hướng_dẫn', 'của', 'Bộ', 'Y_tế', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORGANIZATION', 'I-ORGANIZATION', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "PAD = \"<pad>\"\n",
        "UNK = \"<unk>\"\n",
        "\n",
        "counter = Counter()\n",
        "for sent in train_tokens:\n",
        "    counter.update(sent)\n",
        "\n",
        "idx2word = [PAD, UNK] + list(counter.keys())\n",
        "word2idx = {w:i for i,w in enumerate(idx2word)}\n",
        "vocab_size = len(idx2word)\n",
        "\n",
        "all_tags = sorted({t for sent in train_tags for t in sent})\n",
        "tag2idx = {t:i for i,t in enumerate(all_tags)}\n",
        "idx2tag = {i:t for t,i in tag2idx.items()}\n",
        "num_tags = len(all_tags)\n",
        "\n",
        "vocab_size, num_tags, all_tags[:15]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A8g0CLe0dyc",
        "outputId": "f5fdbb1f-002d-4aba-90ad-905e9224ab94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5243,\n",
              " 20,\n",
              " ['B-AGE',\n",
              "  'B-DATE',\n",
              "  'B-GENDER',\n",
              "  'B-JOB',\n",
              "  'B-LOCATION',\n",
              "  'B-NAME',\n",
              "  'B-ORGANIZATION',\n",
              "  'B-PATIENT_ID',\n",
              "  'B-SYMPTOM_AND_DISEASE',\n",
              "  'B-TRANSPORTATION',\n",
              "  'I-AGE',\n",
              "  'I-DATE',\n",
              "  'I-JOB',\n",
              "  'I-LOCATION',\n",
              "  'I-NAME'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "MAX_LEN = 160\n",
        "\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sents_tokens, sents_tags, word2idx, tag2idx, max_len=160):\n",
        "        self.X = sents_tokens\n",
        "        self.Y = sents_tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        tokens = self.X[i][:self.max_len]\n",
        "        tags = self.Y[i][:self.max_len]\n",
        "        ids = [self.word2idx.get(w, self.word2idx[UNK]) for w in tokens]\n",
        "        y = [self.tag2idx[t] for t in tags]\n",
        "        return ids, y\n",
        "\n",
        "def collate_fn(batch):\n",
        "    pad_id = word2idx[PAD]\n",
        "    pad_tag = -100  # ignore_index cho CrossEntropyLoss\n",
        "\n",
        "    maxl = max(len(x[0]) for x in batch)\n",
        "    input_ids, attn_mask, labels = [], [], []\n",
        "\n",
        "    for ids, y in batch:\n",
        "        pad_len = maxl - len(ids)\n",
        "        input_ids.append(ids + [pad_id]*pad_len)\n",
        "        attn_mask.append([1]*len(ids) + [0]*pad_len)\n",
        "        labels.append(y + [pad_tag]*pad_len)\n",
        "\n",
        "    return (\n",
        "        torch.tensor(input_ids, dtype=torch.long),\n",
        "        torch.tensor(attn_mask, dtype=torch.long),\n",
        "        torch.tensor(labels, dtype=torch.long),\n",
        "    )\n"
      ],
      "metadata": {
        "id": "OBIy1niM0f4Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(NERDataset(train_tokens, train_tags, word2idx, tag2idx, MAX_LEN),\n",
        "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "dev_loader   = DataLoader(NERDataset(dev_tokens, dev_tags, word2idx, tag2idx, MAX_LEN),\n",
        "                          batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(NERDataset(test_tokens, test_tags, word2idx, tag2idx, MAX_LEN),\n",
        "                          batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "len(train_loader), len(dev_loader), len(test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIKafcwD0h9C",
        "outputId": "31c4ff03-736e-4872-a8b0-6fb9072a686d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(158, 63, 94)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # [1,T,D]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "Dz6EtXsw0ji9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerEncoderTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, num_tags, pad_id,\n",
        "                 d_model=256, nhead=8, dim_ff=1024, num_layers=3, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
        "        self.pos = PositionalEncoding(d_model, dropout=dropout, max_len=512)\n",
        "\n",
        "        enc_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_ff,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(d_model, num_tags)\n",
        "\n",
        "    def forward(self, input_ids, attn_mask):\n",
        "        # input_ids: [B,T], attn_mask: [B,T]\n",
        "        x = self.emb(input_ids) * math.sqrt(self.d_model)\n",
        "        x = self.pos(x)\n",
        "\n",
        "        src_key_padding_mask = (attn_mask == 0)  # True ở PAD\n",
        "        x = self.encoder(x, src_key_padding_mask=src_key_padding_mask)  # [B,T,D]\n",
        "        logits = self.classifier(x)  # [B,T,C]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ik2rGuEf0lfZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TransformerEncoderTagger(\n",
        "    vocab_size=vocab_size,\n",
        "    num_tags=num_tags,\n",
        "    pad_id=word2idx[PAD],\n",
        "    d_model=256,\n",
        "    nhead=8,\n",
        "    dim_ff=1024,\n",
        "    num_layers=3,   # đúng yêu cầu 3 lớp\n",
        "    dropout=0.1\n",
        ").to(DEVICE)\n",
        "\n",
        "sum(p.numel() for p in model.parameters())/1e6\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRN3K9II0nfY",
        "outputId": "96373899-725b-42b4-ca7e-b5a9ba54d557"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.716628"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 3e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "def bio_to_spans(tags):\n",
        "    # tags: list[str] BIO (ví dụ B-LOC, I-LOC, O)\n",
        "    spans = []\n",
        "    start, ent_type = None, None\n",
        "    for i, t in enumerate(tags + [\"O\"]):  # sentinel\n",
        "        if t == \"O\" or t == \"PAD\":\n",
        "            if start is not None:\n",
        "                spans.append((start, i-1, ent_type))\n",
        "                start, ent_type = None, None\n",
        "            continue\n",
        "        if t.startswith(\"B-\"):\n",
        "            if start is not None:\n",
        "                spans.append((start, i-1, ent_type))\n",
        "            start = i\n",
        "            ent_type = t[2:]\n",
        "        elif t.startswith(\"I-\"):\n",
        "            typ = t[2:]\n",
        "            if start is None or typ != ent_type:\n",
        "                # I- bị lỗi -> coi như B-\n",
        "                if start is not None:\n",
        "                    spans.append((start, i-1, ent_type))\n",
        "                start = i\n",
        "                ent_type = typ\n",
        "    return spans\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_entity_f1(model, loader):\n",
        "    model.eval()\n",
        "    tp = fp = fn = 0\n",
        "    token_correct = token_total = 0\n",
        "    total_loss = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for input_ids, attn_mask, labels in loader:\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attn_mask = attn_mask.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        logits = model(input_ids, attn_mask)\n",
        "        loss = criterion(logits.view(-1, num_tags), labels.view(-1))\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "        n_samples += input_ids.size(0)\n",
        "\n",
        "        preds = logits.argmax(-1)  # [B,T]\n",
        "        for b in range(input_ids.size(0)):\n",
        "            # lấy phần token thật (attn_mask=1)\n",
        "            L = int(attn_mask[b].sum().item())\n",
        "            gold_ids = labels[b][:L].tolist()\n",
        "            pred_ids = preds[b][:L].tolist()\n",
        "\n",
        "            gold_tags = [idx2tag[i] for i in gold_ids]\n",
        "            pred_tags = [idx2tag[i] for i in pred_ids]\n",
        "\n",
        "            # token-acc\n",
        "            token_correct += sum(g==p for g,p in zip(gold_ids, pred_ids))\n",
        "            token_total += L\n",
        "\n",
        "            gold_spans = set(bio_to_spans(gold_tags))\n",
        "            pred_spans = set(bio_to_spans(pred_tags))\n",
        "\n",
        "            tp += len(gold_spans & pred_spans)\n",
        "            fp += len(pred_spans - gold_spans)\n",
        "            fn += len(gold_spans - pred_spans)\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-9)\n",
        "    recall    = tp / (tp + fn + 1e-9)\n",
        "    f1        = 2*precision*recall / (precision + recall + 1e-9)\n",
        "    token_acc = token_correct / max(1, token_total)\n",
        "    return total_loss/max(1,n_samples), token_acc, precision, recall, f1\n"
      ],
      "metadata": {
        "id": "Dvm6Hd4-0pwd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_samples = 0\n",
        "\n",
        "    for input_ids, attn_mask, labels in loader:\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attn_mask = attn_mask.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(input_ids, attn_mask)\n",
        "        loss = criterion(logits.view(-1, num_tags), labels.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "        n_samples += input_ids.size(0)\n",
        "\n",
        "    return total_loss / max(1, n_samples)\n"
      ],
      "metadata": {
        "id": "4LfXzwfk0sc4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "PATIENCE = 3\n",
        "best_path = \"best_ner_encoder3.pt\"\n",
        "\n",
        "best_f1 = -1.0\n",
        "patience_left = PATIENCE\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr_loss = train_one_epoch(model, train_loader)\n",
        "    dev_loss, dev_tok_acc, dev_p, dev_r, dev_f1 = eval_entity_f1(model, dev_loader)\n",
        "\n",
        "    improved = dev_f1 > best_f1 + 1e-4\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train_loss={tr_loss:.4f} | dev_loss={dev_loss:.4f} | \"\n",
        "        f\"dev_tok_acc={dev_tok_acc:.4f} | dev_P={dev_p:.4f} | dev_R={dev_r:.4f} | dev_F1={dev_f1:.4f} | \"\n",
        "        f\"{'improved' if improved else 'no-improve'} | patience_left={patience_left}\"\n",
        "    )\n",
        "\n",
        "    if improved:\n",
        "        best_f1 = dev_f1\n",
        "        patience_left = PATIENCE\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "    else:\n",
        "        patience_left -= 1\n",
        "        if patience_left == 0:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "print(\"Best dev F1:\", best_f1)\n",
        "print(\"Saved:\", best_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlfERKIt0ulE",
        "outputId": "7b2ac614-f357-42c4-f0ca-713c85c8dad7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train_loss=0.6185 | dev_loss=0.5051 | dev_tok_acc=0.8538 | dev_P=0.3762 | dev_R=0.4194 | dev_F1=0.3966 | improved | patience_left=3\n",
            "Epoch 02 | train_loss=0.3329 | dev_loss=0.3945 | dev_tok_acc=0.8827 | dev_P=0.4879 | dev_R=0.5516 | dev_F1=0.5178 | improved | patience_left=3\n",
            "Epoch 03 | train_loss=0.2562 | dev_loss=0.3594 | dev_tok_acc=0.8942 | dev_P=0.5174 | dev_R=0.6253 | dev_F1=0.5662 | improved | patience_left=3\n",
            "Epoch 04 | train_loss=0.2132 | dev_loss=0.3384 | dev_tok_acc=0.8990 | dev_P=0.5562 | dev_R=0.6999 | dev_F1=0.6198 | improved | patience_left=3\n",
            "Epoch 05 | train_loss=0.1818 | dev_loss=0.3519 | dev_tok_acc=0.9055 | dev_P=0.5812 | dev_R=0.6539 | dev_F1=0.6154 | no-improve | patience_left=3\n",
            "Epoch 06 | train_loss=0.1626 | dev_loss=0.3219 | dev_tok_acc=0.9095 | dev_P=0.6007 | dev_R=0.6906 | dev_F1=0.6425 | improved | patience_left=2\n",
            "Epoch 07 | train_loss=0.1456 | dev_loss=0.3220 | dev_tok_acc=0.9085 | dev_P=0.5807 | dev_R=0.7133 | dev_F1=0.6402 | no-improve | patience_left=3\n",
            "Epoch 08 | train_loss=0.1345 | dev_loss=0.3456 | dev_tok_acc=0.9124 | dev_P=0.6186 | dev_R=0.7051 | dev_F1=0.6590 | improved | patience_left=2\n",
            "Epoch 09 | train_loss=0.1226 | dev_loss=0.3192 | dev_tok_acc=0.9137 | dev_P=0.6039 | dev_R=0.7355 | dev_F1=0.6632 | improved | patience_left=3\n",
            "Epoch 10 | train_loss=0.1112 | dev_loss=0.3683 | dev_tok_acc=0.9156 | dev_P=0.6264 | dev_R=0.7144 | dev_F1=0.6675 | improved | patience_left=3\n",
            "Epoch 11 | train_loss=0.1029 | dev_loss=0.3427 | dev_tok_acc=0.9167 | dev_P=0.6262 | dev_R=0.7047 | dev_F1=0.6631 | no-improve | patience_left=3\n",
            "Epoch 12 | train_loss=0.0961 | dev_loss=0.3426 | dev_tok_acc=0.9159 | dev_P=0.6275 | dev_R=0.7336 | dev_F1=0.6764 | improved | patience_left=2\n",
            "Epoch 13 | train_loss=0.0893 | dev_loss=0.3461 | dev_tok_acc=0.9170 | dev_P=0.6198 | dev_R=0.7197 | dev_F1=0.6660 | no-improve | patience_left=3\n",
            "Epoch 14 | train_loss=0.0833 | dev_loss=0.3570 | dev_tok_acc=0.9180 | dev_P=0.6406 | dev_R=0.7243 | dev_F1=0.6799 | improved | patience_left=2\n",
            "Epoch 15 | train_loss=0.0787 | dev_loss=0.3512 | dev_tok_acc=0.9186 | dev_P=0.6271 | dev_R=0.7328 | dev_F1=0.6758 | no-improve | patience_left=3\n",
            "Epoch 16 | train_loss=0.0739 | dev_loss=0.3784 | dev_tok_acc=0.9181 | dev_P=0.6337 | dev_R=0.7132 | dev_F1=0.6711 | no-improve | patience_left=2\n",
            "Epoch 17 | train_loss=0.0701 | dev_loss=0.4086 | dev_tok_acc=0.9184 | dev_P=0.6269 | dev_R=0.7168 | dev_F1=0.6688 | no-improve | patience_left=1\n",
            "Early stopping triggered!\n",
            "Best dev F1: 0.6798895300064358\n",
            "Saved: best_ner_encoder3.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
        "test_loss, test_tok_acc, test_p, test_r, test_f1 = eval_entity_f1(model, test_loader)\n",
        "\n",
        "print(\"TEST loss:\", test_loss)\n",
        "print(\"TEST token-acc:\", test_tok_acc)\n",
        "print(\"TEST P/R/F1:\", test_p, test_r, test_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UU4u9sn0zIy",
        "outputId": "84d9c98c-d4a1-46e5-baf7-6c5c3b43ddff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST loss: 0.40404242599010465\n",
            "TEST token-acc: 0.9111419767496148\n",
            "TEST P/R/F1: 0.6255040706078806 0.70055389859389 0.6609052169627591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval_entity_f1_per_type(model, loader):\n",
        "    model.eval()\n",
        "    stats = {}  # type -> tp, fp, fn\n",
        "\n",
        "    for input_ids, attn_mask, labels in loader:\n",
        "        input_ids = input_ids.to(DEVICE)\n",
        "        attn_mask = attn_mask.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        logits = model(input_ids, attn_mask)\n",
        "        preds = logits.argmax(-1)\n",
        "\n",
        "        for b in range(input_ids.size(0)):\n",
        "            L = int(attn_mask[b].sum().item())\n",
        "            gold_tags = [idx2tag[i] for i in labels[b][:L].tolist()]\n",
        "            pred_tags = [idx2tag[i] for i in preds[b][:L].tolist()]\n",
        "\n",
        "            gold_spans = bio_to_spans(gold_tags)\n",
        "            pred_spans = bio_to_spans(pred_tags)\n",
        "\n",
        "            gold_by_type = {}\n",
        "            pred_by_type = {}\n",
        "            for s,e,t in gold_spans:\n",
        "                gold_by_type.setdefault(t,set()).add((s,e))\n",
        "            for s,e,t in pred_spans:\n",
        "                pred_by_type.setdefault(t,set()).add((s,e))\n",
        "\n",
        "            all_types = set(gold_by_type) | set(pred_by_type)\n",
        "            for t in all_types:\n",
        "                g = gold_by_type.get(t,set())\n",
        "                p = pred_by_type.get(t,set())\n",
        "                tp = len(g & p)\n",
        "                fp = len(p - g)\n",
        "                fn = len(g - p)\n",
        "                if t not in stats:\n",
        "                    stats[t] = [0,0,0]\n",
        "                stats[t][0] += tp\n",
        "                stats[t][1] += fp\n",
        "                stats[t][2] += fn\n",
        "\n",
        "    # print\n",
        "    print(\"=\"*72)\n",
        "    print(f\"{'Type':<15} {'Precision':>10} {'Recall':>10} {'F1':>10} {'Support':>10}\")\n",
        "    print(\"-\"*72)\n",
        "\n",
        "    for t in sorted(stats.keys()):\n",
        "        tp, fp, fn = stats[t]\n",
        "        p = tp / (tp + fp + 1e-9)\n",
        "        r = tp / (tp + fn + 1e-9)\n",
        "        f1 = 2*p*r / (p+r+1e-9)\n",
        "        support = tp + fn\n",
        "        print(f\"{t:<15} {p:10.4f} {r:10.4f} {f1:10.4f} {support:10d}\")\n",
        "\n",
        "    print(\"=\"*72)\n",
        "\n",
        "eval_entity_f1_per_type(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEJl37gl019I",
        "outputId": "eb27e03e-8d4f-494c-d33f-7d7080f12781"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================\n",
            "Type             Precision     Recall         F1    Support\n",
            "------------------------------------------------------------------------\n",
            "AGE                 0.7457     0.8866     0.8100        582\n",
            "DATE                0.5630     0.7648     0.6486       1654\n",
            "GENDER              0.8788     0.9416     0.9091        462\n",
            "JOB                 0.4659     0.4740     0.4699        173\n",
            "LOCATION            0.5991     0.6609     0.6285       4441\n",
            "NAME                0.8384     0.5220     0.6434        318\n",
            "ORGANIZATION        0.3590     0.5979     0.4487        771\n",
            "PATIENT_ID          0.8526     0.7676     0.8079       2005\n",
            "SYMPTOM_AND_DISEASE     0.5951     0.6417     0.6175       1136\n",
            "TRANSPORTATION      0.7623     0.4819     0.5905        193\n",
            "========================================================================\n"
          ]
        }
      ]
    }
  ]
}