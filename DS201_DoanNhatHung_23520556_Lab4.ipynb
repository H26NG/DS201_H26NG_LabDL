{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Bài 1: Xây dựng kiến trúc Encoder-Decoder gồm 3 lớp LSTM cho module encoder và 3 lớp LSTM cho module decoder, với hidden size là 256, cho bài toán dịch máy từ tiếng Anh sang tiếng Việt. Huấn luyện mô hình này trên bộ dữ liệu PhoMT sử dụng Adam làm phương thức tối ưu tham số. Đánh giá độ hiệu quả của mô hình sử dụng độ đo ROUGE-L."
      ],
      "metadata": {
        "id": "L-sBR3_5P4mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORT THƯ VIỆN"
      ],
      "metadata": {
        "id": "-30VcXxFP-zF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9YjxMHwPzX_",
        "outputId": "4d40dafc-b67a-400b-823d-116c34759cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.9.0+cu126\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"/content/small-train.json\"\n",
        "DEV_PATH   = \"/content/small-dev.json\"\n",
        "TEST_PATH  = \"/content/small-test.json\"\n"
      ],
      "metadata": {
        "id": "IlPD3WDcQQ-r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Khảo sát dữ liệu"
      ],
      "metadata": {
        "id": "jdOvrDofQZzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_tokenize(s: str) -> List[str]:\n",
        "    return s.strip().split()\n",
        "\n",
        "def dataset_stats(path: str, n_show: int = 3):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"\\n[EDA] File: {path}\")\n",
        "    print(\"[EDA] Num samples:\", len(data))\n",
        "\n",
        "    src_lens, tgt_lens = [], []\n",
        "    src_all, tgt_all = [], []\n",
        "\n",
        "    for ex in data:\n",
        "        s = basic_tokenize(ex[\"english\"])\n",
        "        t = basic_tokenize(ex[\"vietnamese\"])\n",
        "        src_lens.append(len(s)); tgt_lens.append(len(t))\n",
        "        src_all += s; tgt_all += t\n",
        "\n",
        "    def summarize(lens):\n",
        "        lens = sorted(lens)\n",
        "        mean = sum(lens)/max(1,len(lens))\n",
        "        p50 = lens[len(lens)//2] if lens else 0\n",
        "        p90 = lens[int(0.9*len(lens))-1] if len(lens) > 0 else 0\n",
        "        mx  = max(lens) if lens else 0\n",
        "        return mean, p50, p90, mx\n",
        "\n",
        "    sm, s50, s90, smax = summarize(src_lens)\n",
        "    tm, t50, t90, tmax = summarize(tgt_lens)\n",
        "\n",
        "    print(f\"[EDA] EN len: mean={sm:.2f}, p50={s50}, p90={s90}, max={smax}\")\n",
        "    print(f\"[EDA] VI len: mean={tm:.2f}, p50={t50}, p90={t90}, max={tmax}\")\n",
        "\n",
        "    print(\"[EDA] Top EN tokens:\", Counter(src_all).most_common(10))\n",
        "    print(\"[EDA] Top VI tokens:\", Counter(tgt_all).most_common(10))\n",
        "\n",
        "    print(\"\\n[EDA] Samples:\")\n",
        "    for i in range(min(n_show, len(data))):\n",
        "        print(f\"- EN: {data[i]['english']}\")\n",
        "        print(f\"  VI: {data[i]['vietnamese']}\")\n",
        "\n",
        "dataset_stats(TRAIN_PATH, n_show=3)\n",
        "dataset_stats(DEV_PATH, n_show=2)\n",
        "dataset_stats(TEST_PATH, n_show=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gNAkFVyQWxT",
        "outputId": "b831262d-9a83-49b8-e7b7-d2817b3655f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[EDA] File: /content/small-train.json\n",
            "[EDA] Num samples: 20000\n",
            "[EDA] EN len: mean=19.79, p50=16, p90=37, max=164\n",
            "[EDA] VI len: mean=23.76, p50=19, p90=45, max=179\n",
            "[EDA] Top EN tokens: [(',', 23180), ('.', 18302), ('the', 14556), ('to', 9433), ('of', 8532), ('and', 8242), ('a', 7647), ('that', 7083), ('I', 5973), ('in', 5556)]\n",
            "[EDA] Top VI tokens: [(',', 19699), ('.', 16254), ('là', 8091), ('tôi', 7122), ('và', 7024), ('có', 6977), ('một', 6598), ('chúng', 5131), ('những', 5037), ('của', 4960)]\n",
            "\n",
            "[EDA] Samples:\n",
            "- EN: It begins with a countdown .\n",
            "  VI: Câu chuyện bắt đầu với buổi lễ đếm ngược .\n",
            "- EN: On August 14th , 1947 , a woman in Bombay goes into labor as the clock ticks towards midnight .\n",
            "  VI: Ngày 14 , tháng 8 , năm 1947 , gần nửa đêm , ở Bombay , có một phụ nữ sắp lâm bồn .\n",
            "- EN: Across India , people hold their breath for the declaration of independence after nearly two centuries of British occupation and rule .\n",
            "  VI: Cùng lúc , trên khắp đất Ấn , người ta nín thở chờ đợi tuyên ngôn độc lập sau gần hai thập kỷ là thuộc địa của Anh .\n",
            "\n",
            "[EDA] File: /content/small-dev.json\n",
            "[EDA] Num samples: 2000\n",
            "[EDA] EN len: mean=20.01, p50=18, p90=34, max=154\n",
            "[EDA] VI len: mean=25.03, p50=23, p90=43, max=184\n",
            "[EDA] Top EN tokens: [(',', 2170), ('.', 1872), ('the', 1573), ('of', 995), ('to', 959), ('and', 863), ('a', 814), ('in', 616), ('that', 509), ('I', 423)]\n",
            "[EDA] Top VI tokens: [(',', 1979), ('.', 1791), ('và', 754), ('một', 723), ('là', 710), ('có', 655), ('của', 651), ('tôi', 581), ('những', 466), ('chúng', 391)]\n",
            "\n",
            "[EDA] Samples:\n",
            "- EN: ﻿Hurricane Dorian , one of the most powerful storms ever recorded in the Atlantic Ocean , made landfall as a Category 5 storm on Great Abaco Island in the northern Bahamas on Sunday morning , September 1 , 2019 .\n",
            "  VI: Vào chủ nhật ngày 1-9-2019 , cơn bão Dorian , một trong những cơn bão mạnh nhất được ghi nhận ở Đại Tây Dương , với sức gió 362 km/h đổ bộ vào đảo Great Abaco , miền bắc Bahamas .\n",
            "- EN: Dorian is especially dangerous due to its slow movement , high wind speeds , and heavy rains .\n",
            "  VI: Bão Dorian đặc biệt nguy hiểm vì nó di chuyển chậm , có tốc độ gió cao và gây mưa lớn .\n",
            "\n",
            "[EDA] File: /content/small-test.json\n",
            "[EDA] Num samples: 2000\n",
            "[EDA] EN len: mean=22.62, p50=20, p90=40, max=88\n",
            "[EDA] VI len: mean=28.79, p50=25, p90=51, max=130\n",
            "[EDA] Top EN tokens: [(',', 2488), ('the', 1907), ('.', 1882), ('of', 1202), ('to', 1155), ('and', 993), ('a', 828), ('in', 745), ('that', 714), ('is', 500)]\n",
            "[EDA] Top VI tokens: [(',', 2176), ('.', 1835), ('và', 944), ('là', 856), ('có', 817), ('một', 801), ('những', 786), ('của', 688), ('chúng', 536), ('người', 468)]\n",
            "\n",
            "[EDA] Samples:\n",
            "- EN: Brother Albert Barnett and his wife , Sister Susan Barnett , from the West Congregation in Tuscaloosa , Alabama\n",
            "  VI: Anh Albert Barnett và chị Susan Barnett , thuộc hội thánh West ở Tuscaloosa , Alabama\n",
            "- EN: Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12 , 2020 .\n",
            "  VI: Ngày 11 và 12-1-2020 , những cơn bão lớn đã quét qua và phá huỷ nhiều vùng ở miền nam và miền trung Hoa Kỳ .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Xây dựng vocab"
      ],
      "metadata": {
        "id": "eccESTxmQkSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SPECIALS = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "PAD, UNK, BOS, EOS = SPECIALS\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokenized_sents: List[List[str]], min_freq=1, max_size=50000):\n",
        "        freq = Counter()\n",
        "        for sent in tokenized_sents:\n",
        "            freq.update(sent)\n",
        "\n",
        "        items = [(w,c) for w,c in freq.items() if c >= min_freq]\n",
        "        items.sort(key=lambda x: (-x[1], x[0]))\n",
        "        items = items[: max(0, max_size - len(SPECIALS))]\n",
        "\n",
        "        self.itos = SPECIALS + [w for w,_ in items]\n",
        "        self.stoi = {w:i for i,w in enumerate(self.itos)}\n",
        "        self.pad_id = self.stoi[PAD]\n",
        "        self.unk_id = self.stoi[UNK]\n",
        "        self.bos_id = self.stoi[BOS]\n",
        "        self.eos_id = self.stoi[EOS]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def encode(self, toks: List[str]) -> List[int]:\n",
        "        return [self.stoi.get(t, self.unk_id) for t in toks]\n",
        "\n",
        "    def decode(self, ids: List[int]) -> List[str]:\n",
        "        out = []\n",
        "        for i in ids:\n",
        "            if i == self.eos_id: break\n",
        "            if i in (self.pad_id, self.bos_id): continue\n",
        "            out.append(self.itos[i] if 0 <= i < len(self.itos) else UNK)\n",
        "        return out\n",
        "\n",
        "class PhoMTJsonDataset(Dataset):\n",
        "    def __init__(self, path: str):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            self.data = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        ex = self.data[idx]\n",
        "        return ex[\"english\"], ex[\"vietnamese\"]\n",
        "\n",
        "def build_vocabs(train_path: str, min_freq=1, max_size=50000):\n",
        "    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    src_tok = [basic_tokenize(x[\"english\"]) for x in data]\n",
        "    tgt_tok = [basic_tokenize(x[\"vietnamese\"]) for x in data]\n",
        "    return Vocab(src_tok, min_freq=min_freq, max_size=max_size), Vocab(tgt_tok, min_freq=min_freq, max_size=max_size)\n",
        "\n",
        "@dataclass\n",
        "class Batch:\n",
        "    src: torch.Tensor\n",
        "    tgt_in: torch.Tensor\n",
        "    tgt_out: torch.Tensor\n",
        "\n",
        "def pad_2d(seqs: List[torch.Tensor], pad_id: int) -> torch.Tensor:\n",
        "    maxlen = max(s.size(0) for s in seqs)\n",
        "    out = torch.full((len(seqs), maxlen), pad_id, dtype=torch.long)\n",
        "    for i,s in enumerate(seqs):\n",
        "        out[i, :s.size(0)] = s\n",
        "    return out\n",
        "\n",
        "MAX_LEN_SRC = 170\n",
        "MAX_LEN_TGT = 190\n",
        "\n",
        "def collate_fn(batch_items, src_vocab: Vocab, tgt_vocab: Vocab):\n",
        "    src_list, tgt_in_list, tgt_out_list = [], [], []\n",
        "    for en, vi in batch_items:\n",
        "        src_toks = basic_tokenize(en)[:MAX_LEN_SRC]\n",
        "        tgt_toks = basic_tokenize(vi)[:MAX_LEN_TGT]\n",
        "        src_ids = src_vocab.encode(src_toks)\n",
        "        tgt_ids = tgt_vocab.encode(tgt_toks)\n",
        "        tgt_in  = [tgt_vocab.bos_id] + tgt_ids\n",
        "        tgt_out = tgt_ids + [tgt_vocab.eos_id]\n",
        "        src_list.append(torch.tensor(src_ids, dtype=torch.long))\n",
        "        tgt_in_list.append(torch.tensor(tgt_in, dtype=torch.long))\n",
        "        tgt_out_list.append(torch.tensor(tgt_out, dtype=torch.long))\n",
        "\n",
        "    src = pad_2d(src_list, src_vocab.pad_id)\n",
        "    tgt_in = pad_2d(tgt_in_list, tgt_vocab.pad_id)\n",
        "    tgt_out = pad_2d(tgt_out_list, tgt_vocab.pad_id)\n",
        "    return Batch(src=src, tgt_in=tgt_in, tgt_out=tgt_out)\n",
        "\n",
        "src_vocab, tgt_vocab = build_vocabs(TRAIN_PATH, min_freq=1, max_size=50000)\n",
        "print(\"src_vocab:\", len(src_vocab))\n",
        "print(\"tgt_vocab:\", len(tgt_vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6U9JstfQha6",
        "outputId": "6d5d182c-a7ce-436d-921f-6b2b9a297974"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_vocab: 19065\n",
            "tgt_vocab: 8297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Xây dựng mô hình encoderdecoder"
      ],
      "metadata": {
        "id": "eDG62BL0Qrq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_DIM = 256\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.2\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, pad_id: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=EMBED_DIM,\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            dropout=DROPOUT if NUM_LAYERS > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, src_ids):\n",
        "        # src_ids: [B,S]\n",
        "        x = self.emb(src_ids)      # [B,S,256]\n",
        "        _, (h, c) = self.lstm(x)   # h,c: [3,B,256]\n",
        "        return h, c\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, pad_id: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=EMBED_DIM,\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            dropout=DROPOUT if NUM_LAYERS > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.proj = nn.Linear(HIDDEN_SIZE, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_in_ids, h, c):\n",
        "        # tgt_in_ids: [B,T]\n",
        "        x = self.emb(tgt_in_ids)        # [B,T,256]\n",
        "        out, (h, c) = self.lstm(x, (h,c))  # out: [B,T,256]\n",
        "        logits = self.proj(out)         # [B,T,V]\n",
        "        return logits, h, c\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, enc: Encoder, dec: Decoder, pad_id: int):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "\n",
        "    def forward(self, src_ids, tgt_in, tgt_out):\n",
        "        h, c = self.enc(src_ids)\n",
        "        logits, _, _ = self.dec(tgt_in, h, c)\n",
        "        loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        return loss\n",
        "\n",
        "enc = Encoder(len(src_vocab), src_vocab.pad_id)\n",
        "dec = Decoder(len(tgt_vocab), tgt_vocab.pad_id)\n",
        "model = Seq2Seq(enc, dec, pad_id=tgt_vocab.pad_id).to(DEVICE)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cFwHE9hQno-",
        "outputId": "79055601-ace7-4d4a-a0fd-421e296b9fed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2Seq(\n",
            "  (enc): Encoder(\n",
            "    (emb): Embedding(19065, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (dec): Decoder(\n",
            "    (emb): Embedding(8297, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
            "    (proj): Linear(in_features=256, out_features=8297, bias=True)\n",
            "  )\n",
            "  (loss_fn): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataLoader + kiểm tra 1 batch"
      ],
      "metadata": {
        "id": "Z7yW0pl0RKpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = PhoMTJsonDataset(TRAIN_PATH)\n",
        "dev_ds   = PhoMTJsonDataset(DEV_PATH)\n",
        "test_ds  = PhoMTJsonDataset(TEST_PATH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,\n",
        "                          collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab))\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=32, shuffle=False,\n",
        "                          collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab))\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False,\n",
        "                          collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab))\n",
        "\n",
        "batch = next(iter(train_loader))\n",
        "print(\"src:\", batch.src.shape)\n",
        "print(\"tgt_in:\", batch.tgt_in.shape)\n",
        "print(\"tgt_out:\", batch.tgt_out.shape)\n",
        "\n",
        "loss = model(batch.src.to(DEVICE), batch.tgt_in.to(DEVICE), batch.tgt_out.to(DEVICE))\n",
        "print(\"sanity loss:\", float(loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsslnbihRLYo",
        "outputId": "3621e7a4-559a-4bc2-e164-014e4ce63222"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: torch.Size([32, 51])\n",
            "tgt_in: torch.Size([32, 56])\n",
            "tgt_out: torch.Size([32, 56])\n",
            "sanity loss: 9.021029472351074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def greedy_decode(model: Seq2Seq, src_ids: torch.Tensor, tgt_vocab: Vocab, max_len= MAX_LEN_TGT):\n",
        "    model.eval()\n",
        "    h, c = model.enc(src_ids)\n",
        "    B = src_ids.size(0)\n",
        "    ys = torch.full((B,1), tgt_vocab.bos_id, dtype=torch.long, device=src_ids.device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        logits, h, c = model.dec(ys, h, c)\n",
        "        next_tok = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "        ys = torch.cat([ys, next_tok], dim=1)\n",
        "        if (next_tok.squeeze(1) == tgt_vocab.eos_id).all():\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "def lcs_length(a: List[str], b: List[str]) -> int:\n",
        "    n, m = len(a), len(b)\n",
        "    dp = [0]*(m+1)\n",
        "    for i in range(1, n+1):\n",
        "        prev = 0\n",
        "        for j in range(1, m+1):\n",
        "            tmp = dp[j]\n",
        "            if a[i-1] == b[j-1]:\n",
        "                dp[j] = prev + 1\n",
        "            else:\n",
        "                dp[j] = max(dp[j], dp[j-1])\n",
        "            prev = tmp\n",
        "    return dp[m]\n",
        "\n",
        "def rouge_l_f1(pred: List[str], ref: List[str]) -> float:\n",
        "    if not pred or not ref:\n",
        "        return 0.0\n",
        "    lcs = lcs_length(pred, ref)\n",
        "    p = lcs / max(1, len(pred))\n",
        "    r = lcs / max(1, len(ref))\n",
        "    return 0.0 if (p+r)==0 else (2*p*r)/(p+r)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_rouge_l(model: Seq2Seq, loader: DataLoader, tgt_vocab: Vocab):\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    for batch in loader:\n",
        "        src = batch.src.to(DEVICE)\n",
        "        pred_ids = greedy_decode(model, src, tgt_vocab, max_len=MAX_LEN_TGT)[:, 1:].tolist()  # drop BOS\n",
        "        for i in range(len(pred_ids)):\n",
        "            pred_toks = tgt_vocab.decode(pred_ids[i])\n",
        "            ref_toks  = tgt_vocab.decode(batch.tgt_out[i].tolist())\n",
        "            scores.append(rouge_l_f1(pred_toks, ref_toks))\n",
        "    return sum(scores)/max(1,len(scores))\n"
      ],
      "metadata": {
        "id": "k87A0FwoRVyJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Huấn luyện mô hình"
      ],
      "metadata": {
        "id": "ghagddd_ShFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 3e-4\n",
        "EPOCHS = 30\n",
        "GRAD_CLIP = 1.0\n",
        "PATIENCE = 5         # dừng nếu 5 epoch liên tiếp không cải thiện dev ROUGE-L\n",
        "MIN_DELTA = 1e-4     # cải thiện tối thiểu để tính là \"better\"\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "best_dev = -1.0\n",
        "best_epoch = 0\n",
        "bad_epochs = 0\n",
        "BEST_PATH = \"best_lstm3_seq2seq_envi.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "        src = batch.src.to(DEVICE)\n",
        "        tgt_in = batch.tgt_in.to(DEVICE)\n",
        "        tgt_out = batch.tgt_out.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(src, tgt_in, tgt_out)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "    train_loss = total_loss / max(1, len(train_loader))\n",
        "    dev_rouge = evaluate_rouge_l(model, dev_loader, tgt_vocab)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:02d} | train_loss={train_loss:.4f} | dev_ROUGE-L(F1)={dev_rouge:.4f}\")\n",
        "\n",
        "    #Early stopping logic (maximize dev_rouge)\n",
        "    if dev_rouge > best_dev + MIN_DELTA:\n",
        "        best_dev = dev_rouge\n",
        "        best_epoch = epoch\n",
        "        bad_epochs = 0\n",
        "\n",
        "        torch.save({\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"src_itos\": src_vocab.itos,\n",
        "            \"tgt_itos\": tgt_vocab.itos,\n",
        "            \"cfg\": {\"embed_dim\": 256, \"hidden_size\": 256, \"num_layers\": 3,\n",
        "                    \"max_len_src\": MAX_LEN_SRC, \"max_len_tgt\": MAX_LEN_TGT}\n",
        "        }, BEST_PATH)\n",
        "\n",
        "        print(f\"[SAVE] Best model -> {BEST_PATH} (dev_ROUGE-L={best_dev:.4f})\")\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        print(f\"[EARLY STOP] No improvement for {bad_epochs}/{PATIENCE} epochs (best={best_dev:.4f} at epoch {best_epoch})\")\n",
        "\n",
        "        if bad_epochs >= PATIENCE:\n",
        "            print(f\"[EARLY STOP] Stop training. Best dev ROUGE-L={best_dev:.4f} at epoch {best_epoch}.\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFGbRq8kSdBv",
        "outputId": "53f02a0c-d58f-48f7-985a-33fd5238417c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|██████████| 625/625 [00:13<00:00, 45.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01 | train_loss=5.2009 | dev_ROUGE-L(F1)=0.1290\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1290)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 625/625 [00:13<00:00, 46.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 02 | train_loss=5.0063 | dev_ROUGE-L(F1)=0.1347\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1347)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 625/625 [00:13<00:00, 45.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 03 | train_loss=4.8434 | dev_ROUGE-L(F1)=0.1518\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1518)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 625/625 [00:13<00:00, 46.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 04 | train_loss=4.7016 | dev_ROUGE-L(F1)=0.1621\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1621)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 625/625 [00:13<00:00, 46.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 05 | train_loss=4.5757 | dev_ROUGE-L(F1)=0.1689\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1689)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 625/625 [00:13<00:00, 45.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 06 | train_loss=4.4631 | dev_ROUGE-L(F1)=0.1746\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1746)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 625/625 [00:13<00:00, 46.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 07 | train_loss=4.3608 | dev_ROUGE-L(F1)=0.1787\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1787)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 625/625 [00:13<00:00, 46.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 08 | train_loss=4.2695 | dev_ROUGE-L(F1)=0.1821\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1821)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 625/625 [00:13<00:00, 46.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 09 | train_loss=4.1854 | dev_ROUGE-L(F1)=0.1834\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1834)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 625/625 [00:13<00:00, 46.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 | train_loss=4.1073 | dev_ROUGE-L(F1)=0.1857\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1857)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 625/625 [00:13<00:00, 46.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 | train_loss=4.0328 | dev_ROUGE-L(F1)=0.1852\n",
            "[EARLY STOP] No improvement for 1/5 epochs (best=0.1857 at epoch 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 625/625 [00:13<00:00, 46.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 | train_loss=3.9647 | dev_ROUGE-L(F1)=0.1866\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1866)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 625/625 [00:13<00:00, 46.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 | train_loss=3.8999 | dev_ROUGE-L(F1)=0.1892\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1892)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 625/625 [00:13<00:00, 46.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 | train_loss=3.8376 | dev_ROUGE-L(F1)=0.1896\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1896)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 625/625 [00:13<00:00, 46.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 | train_loss=3.7794 | dev_ROUGE-L(F1)=0.1899\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1899)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 625/625 [00:13<00:00, 45.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 | train_loss=3.7232 | dev_ROUGE-L(F1)=0.1905\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1905)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 625/625 [00:13<00:00, 45.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 | train_loss=3.6694 | dev_ROUGE-L(F1)=0.1919\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1919)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 625/625 [00:13<00:00, 45.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 | train_loss=3.6176 | dev_ROUGE-L(F1)=0.1918\n",
            "[EARLY STOP] No improvement for 1/5 epochs (best=0.1919 at epoch 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 625/625 [00:13<00:00, 45.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 | train_loss=3.5691 | dev_ROUGE-L(F1)=0.1906\n",
            "[EARLY STOP] No improvement for 2/5 epochs (best=0.1919 at epoch 17)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 625/625 [00:13<00:00, 45.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 | train_loss=3.5203 | dev_ROUGE-L(F1)=0.1938\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1938)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 625/625 [00:13<00:00, 44.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21 | train_loss=3.4746 | dev_ROUGE-L(F1)=0.1908\n",
            "[EARLY STOP] No improvement for 1/5 epochs (best=0.1938 at epoch 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 625/625 [00:13<00:00, 45.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22 | train_loss=3.4317 | dev_ROUGE-L(F1)=0.1917\n",
            "[EARLY STOP] No improvement for 2/5 epochs (best=0.1938 at epoch 20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 625/625 [00:13<00:00, 46.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23 | train_loss=3.3855 | dev_ROUGE-L(F1)=0.1941\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1941)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 625/625 [00:13<00:00, 45.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24 | train_loss=3.3446 | dev_ROUGE-L(F1)=0.1932\n",
            "[EARLY STOP] No improvement for 1/5 epochs (best=0.1941 at epoch 23)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|██████████| 625/625 [00:13<00:00, 46.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25 | train_loss=3.3048 | dev_ROUGE-L(F1)=0.1955\n",
            "[SAVE] Best model -> best_lstm3_seq2seq_envi.pt (dev_ROUGE-L=0.1955)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|██████████| 625/625 [00:13<00:00, 46.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26 | train_loss=3.2627 | dev_ROUGE-L(F1)=0.1954\n",
            "[EARLY STOP] No improvement for 1/5 epochs (best=0.1955 at epoch 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|██████████| 625/625 [00:13<00:00, 46.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27 | train_loss=3.2254 | dev_ROUGE-L(F1)=0.1931\n",
            "[EARLY STOP] No improvement for 2/5 epochs (best=0.1955 at epoch 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|██████████| 625/625 [00:13<00:00, 46.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28 | train_loss=3.1861 | dev_ROUGE-L(F1)=0.1919\n",
            "[EARLY STOP] No improvement for 3/5 epochs (best=0.1955 at epoch 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|██████████| 625/625 [00:13<00:00, 46.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29 | train_loss=3.1521 | dev_ROUGE-L(F1)=0.1933\n",
            "[EARLY STOP] No improvement for 4/5 epochs (best=0.1955 at epoch 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|██████████| 625/625 [00:13<00:00, 46.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30 | train_loss=3.1191 | dev_ROUGE-L(F1)=0.1935\n",
            "[EARLY STOP] No improvement for 5/5 epochs (best=0.1955 at epoch 25)\n",
            "[EARLY STOP] Stop training. Best dev ROUGE-L=0.1955 at epoch 25.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = torch.load(BEST_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "test_rouge = evaluate_rouge_l(model, test_loader, tgt_vocab)\n",
        "print(\"TEST ROUGE-L(F1) =\", test_rouge)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU0zhz3RTunT",
        "outputId": "b3e5d3f7-df3b-4def-9be4-dffe4aa7d8f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST ROUGE-L(F1) = 0.19458110674019324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for i in range(min(5, len(test_ds))):\n",
        "    en, vi = test_ds[i]\n",
        "    src_ids = torch.tensor([src_vocab.encode(basic_tokenize(en)[:MAX_LEN_SRC])], dtype=torch.long).to(DEVICE)\n",
        "    pred = greedy_decode(model, src_ids, tgt_vocab, max_len=MAX_LEN_TGT)[0].tolist()\n",
        "    pred_text = \" \".join(tgt_vocab.decode(pred[1:]))\n",
        "\n",
        "    print(\"\\nEN:\", en)\n",
        "    print(\"GT:\", vi)\n",
        "    print(\"PR:\", pred_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_jV9nfaTz8Z",
        "outputId": "95695e5b-a6bc-46e3-edd0-db3238cbbef3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EN: Brother Albert Barnett and his wife , Sister Susan Barnett , from the West Congregation in Tuscaloosa , Alabama\n",
            "GT: Anh Albert Barnett và chị Susan Barnett , thuộc hội thánh West ở Tuscaloosa , Alabama\n",
            "PR: Người phụ nữ này , và không .\n",
            "\n",
            "EN: Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12 , 2020 .\n",
            "GT: Ngày 11 và 12-1-2020 , những cơn bão lớn đã quét qua và phá huỷ nhiều vùng ở miền nam và miền trung Hoa Kỳ .\n",
            "PR: Những người nghèo dùng và những người .\n",
            "\n",
            "EN: Two days of heavy rain , high winds , and numerous tornadoes caused major damage across multiple states .\n",
            "GT: Những trận mưa to và gió lớn trong suốt hai ngày cùng với nhiều cơn lốc xoáy đã gây thiệt hại nặng nề cho nhiều bang .\n",
            "PR: Trong vòng 10 năm , chúng ta không .\n",
            "\n",
            "EN: Sadly , Brother Albert Barnett and his wife , Sister Susan Barnett , 85 and 75 years old respectively , were killed when a tornado struck their mobile home .\n",
            "GT: Đáng buồn là anh Albert Barnett 85 tuổi , và vợ anh là chị Susan Barnett 75 tuổi đã thiệt mạng do một cơn lốc xoáy quét qua nhà họ .\n",
            "PR: Những người đã nói chuyện với những người khác .\n",
            "\n",
            "EN: The United States branch also reports that at least four of our brothers ' homes sustained minor damage , along with two Kingdom Halls .\n",
            "GT: Chi nhánh Hoa Kỳ cũng cho biết có ít nhất bốn căn nhà của anh em chúng tôi và hai Phòng Nước Trời bị hư hại nhẹ .\n",
            "PR: Những người đã từng trải qua .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bài 2: Xây dựng kiến trúc Encoder-Decoder gồm 3 lớp LSTM cho module encoder và 3 lớp LSTM cho module decoder, với hidden size là 256, cho bài toán dịch máy từ tiếng Anh sang tiếng Việt. Module decoder được trang bị kỹ thuật attention theo mô tả của nghiên cứu \"[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\". Huấn luyện mô hình này trên bộ dữ liệu PhoMT sử dụng Adam làm phương thức tối ưu tham số. Đánh giá độ hiệu quả của mô hình sử dụn độ đo ROUGE-L."
      ],
      "metadata": {
        "id": "m1yWQ5JQgsVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EMBED_DIM = 256\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.2\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, pad_id: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(\n",
        "            EMBED_DIM, HIDDEN_SIZE,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            dropout=DROPOUT if NUM_LAYERS > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, src_ids):\n",
        "        x = self.emb(src_ids)                 # [B,S,E]\n",
        "        enc_outputs, (h, c) = self.lstm(x)    # [B,S,H], ([L,B,H],[L,B,H])\n",
        "        return enc_outputs, (h, c)\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.W_h = nn.Linear(hidden_size, hidden_size, bias=False)  # encoder side\n",
        "        self.W_s = nn.Linear(hidden_size, hidden_size, bias=False)  # decoder side\n",
        "        self.v   = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, s_t, enc_outputs, src_mask=None):\n",
        "        \"\"\"\n",
        "        s_t: [B,H]  (decoder top-layer hidden at time t)\n",
        "        enc_outputs: [B,S,H]\n",
        "        src_mask: [B,S] True for valid tokens\n",
        "        \"\"\"\n",
        "        # energy: [B,S,H]\n",
        "        energy = torch.tanh(self.W_h(enc_outputs) + self.W_s(s_t).unsqueeze(1))\n",
        "        scores = self.v(energy).squeeze(-1)  # [B,S]\n",
        "        if src_mask is not None:\n",
        "            scores = scores.masked_fill(~src_mask, -1e9)\n",
        "        attn_w = F.softmax(scores, dim=-1)   # [B,S]\n",
        "        context = torch.bmm(attn_w.unsqueeze(1), enc_outputs).squeeze(1)  # [B,H]\n",
        "        return context, attn_w\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, pad_id: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=pad_id)\n",
        "        self.attn = BahdanauAttention(HIDDEN_SIZE)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=EMBED_DIM + HIDDEN_SIZE,\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            dropout=DROPOUT if NUM_LAYERS > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.proj = nn.Linear(HIDDEN_SIZE + HIDDEN_SIZE, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_in_ids, enc_outputs, init_state, src_mask=None):\n",
        "        h, c = init_state\n",
        "        B, T = tgt_in_ids.size()\n",
        "        logits_all = []\n",
        "\n",
        "        for t in range(T):\n",
        "            y_t = tgt_in_ids[:, t]         # [B]\n",
        "            emb_t = self.emb(y_t)          # [B,E]\n",
        "            s_t = h[-1]                    # [B,H] top-layer\n",
        "            context, _ = self.attn(s_t, enc_outputs, src_mask=src_mask)  # [B,H]\n",
        "\n",
        "            lstm_in = torch.cat([emb_t, context], dim=-1).unsqueeze(1)   # [B,1,E+H]\n",
        "            out_t, (h, c) = self.lstm(lstm_in, (h, c))                   # out_t: [B,1,H]\n",
        "            out_t = out_t.squeeze(1)                                     # [B,H]\n",
        "\n",
        "            logits_t = self.proj(torch.cat([out_t, context], dim=-1))    # [B,V]\n",
        "            logits_all.append(logits_t.unsqueeze(1))                     # [B,1,V]\n",
        "\n",
        "        return torch.cat(logits_all, dim=1)  # [B,T,V]\n",
        "\n",
        "class Seq2SeqAttn(nn.Module):\n",
        "    def __init__(self, enc: Encoder, dec: AttnDecoder, pad_id_src: int, pad_id_tgt: int):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "        self.pad_id_src = pad_id_src\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id_tgt)\n",
        "\n",
        "    def make_src_mask(self, src_ids):\n",
        "        return (src_ids != self.pad_id_src)  # [B,S] True for valid\n",
        "\n",
        "    def forward(self, src_ids, tgt_in, tgt_out):\n",
        "        enc_outputs, (h, c) = self.enc(src_ids)\n",
        "        src_mask = self.make_src_mask(src_ids)\n",
        "        logits = self.dec(tgt_in, enc_outputs, (h, c), src_mask=src_mask)\n",
        "        loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        return loss\n",
        "\n",
        "enc = Encoder(len(src_vocab), src_vocab.pad_id)\n",
        "dec = AttnDecoder(len(tgt_vocab), tgt_vocab.pad_id)\n",
        "model = Seq2SeqAttn(enc, dec, pad_id_src=src_vocab.pad_id, pad_id_tgt=tgt_vocab.pad_id).to(DEVICE)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtfQHexdg4Tg",
        "outputId": "7abff4ff-66ce-4613-d276-7bab9dbeccce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2SeqAttn(\n",
            "  (enc): Encoder(\n",
            "    (emb): Embedding(19065, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (dec): AttnDecoder(\n",
            "    (emb): Embedding(8297, 256, padding_idx=0)\n",
            "    (attn): BahdanauAttention(\n",
            "      (W_h): Linear(in_features=256, out_features=256, bias=False)\n",
            "      (W_s): Linear(in_features=256, out_features=256, bias=False)\n",
            "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
            "    )\n",
            "    (lstm): LSTM(512, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
            "    (proj): Linear(in_features=512, out_features=8297, bias=True)\n",
            "  )\n",
            "  (loss_fn): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "loss = model(batch.src.to(DEVICE), batch.tgt_in.to(DEVICE), batch.tgt_out.to(DEVICE))\n",
        "print(\"sanity loss:\", float(loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq8sSSaBht-8",
        "outputId": "c0dbbcf2-03bd-4f26-be83-655da3009524"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanity loss: 9.02454662322998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lcs_length(a: List[str], b: List[str]) -> int:\n",
        "    n, m = len(a), len(b)\n",
        "    dp = [0]*(m+1)\n",
        "    for i in range(1, n+1):\n",
        "        prev = 0\n",
        "        for j in range(1, m+1):\n",
        "            tmp = dp[j]\n",
        "            if a[i-1] == b[j-1]:\n",
        "                dp[j] = prev + 1\n",
        "            else:\n",
        "                dp[j] = max(dp[j], dp[j-1])\n",
        "            prev = tmp\n",
        "    return dp[m]\n",
        "\n",
        "def rouge_l_f1(pred: List[str], ref: List[str]) -> float:\n",
        "    if not pred or not ref:\n",
        "        return 0.0\n",
        "    lcs = lcs_length(pred, ref)\n",
        "    p = lcs / max(1, len(pred))\n",
        "    r = lcs / max(1, len(ref))\n",
        "    return 0.0 if (p+r)==0 else (2*p*r)/(p+r)\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode_attn(model: Seq2SeqAttn, src_ids: torch.Tensor, tgt_vocab: Vocab, max_len=190):\n",
        "    model.eval()\n",
        "    enc_outputs, (h, c) = model.enc(src_ids)\n",
        "    src_mask = (src_ids != model.pad_id_src)\n",
        "\n",
        "    B = src_ids.size(0)\n",
        "    ys = torch.full((B,1), tgt_vocab.bos_id, dtype=torch.long, device=src_ids.device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        y_last = ys[:, -1]                 # [B]\n",
        "        emb_t = model.dec.emb(y_last)      # [B,E]\n",
        "        s_t = h[-1]                        # [B,H]\n",
        "        context, _ = model.dec.attn(s_t, enc_outputs, src_mask=src_mask)\n",
        "\n",
        "        lstm_in = torch.cat([emb_t, context], dim=-1).unsqueeze(1)  # [B,1,E+H]\n",
        "        out_t, (h, c) = model.dec.lstm(lstm_in, (h, c))\n",
        "        out_t = out_t.squeeze(1)                                    # [B,H]\n",
        "\n",
        "        logits = model.dec.proj(torch.cat([out_t, context], dim=-1)) # [B,V]\n",
        "        next_tok = logits.argmax(-1, keepdim=True)                   # [B,1]\n",
        "        ys = torch.cat([ys, next_tok], dim=1)\n",
        "\n",
        "        if (next_tok.squeeze(1) == tgt_vocab.eos_id).all():\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_rouge_l_attn(model, loader, tgt_vocab: Vocab):\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    for batch in loader:\n",
        "        src = batch.src.to(DEVICE)\n",
        "        pred_ids = greedy_decode_attn(model, src, tgt_vocab, max_len=MAX_LEN_TGT)[:, 1:].tolist()\n",
        "        for i in range(len(pred_ids)):\n",
        "            pred_toks = tgt_vocab.decode(pred_ids[i])\n",
        "            ref_toks  = tgt_vocab.decode(batch.tgt_out[i].tolist())\n",
        "            scores.append(rouge_l_f1(pred_toks, ref_toks))\n",
        "    return sum(scores)/max(1,len(scores))\n"
      ],
      "metadata": {
        "id": "r4Ikc2_WidSS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 3e-4\n",
        "EPOCHS = 30\n",
        "GRAD_CLIP = 1.0\n",
        "PATIENCE = 5\n",
        "MIN_DELTA = 1e-4\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "best_dev = -1.0\n",
        "best_epoch = 0\n",
        "bad_epochs = 0\n",
        "BEST_PATH = \"best_lstm3_bahdanau_envi.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "        src = batch.src.to(DEVICE)\n",
        "        tgt_in = batch.tgt_in.to(DEVICE)\n",
        "        tgt_out = batch.tgt_out.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(src, tgt_in, tgt_out)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "    train_loss = total_loss / max(1, len(train_loader))\n",
        "    dev_rouge = evaluate_rouge_l_attn(model, dev_loader, tgt_vocab)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:02d} | train_loss={train_loss:.4f} | dev_ROUGE-L(F1)={dev_rouge:.4f}\")\n",
        "\n",
        "    if dev_rouge > best_dev + MIN_DELTA:\n",
        "        best_dev = dev_rouge\n",
        "        best_epoch = epoch\n",
        "        bad_epochs = 0\n",
        "        torch.save({\"model_state\": model.state_dict()}, BEST_PATH)\n",
        "        print(f\"[SAVE] Best -> {BEST_PATH} (dev_ROUGE-L={best_dev:.4f})\")\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        print(f\"[EARLY STOP] No improvement {bad_epochs}/{PATIENCE} (best={best_dev:.4f} at epoch {best_epoch})\")\n",
        "        if bad_epochs >= PATIENCE:\n",
        "            print(f\"[EARLY STOP] Stop. Best dev ROUGE-L={best_dev:.4f} at epoch {best_epoch}\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3re955Tim-5",
        "outputId": "e572816c-e2a2-489c-c124-7b30cfe8151a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|██████████| 625/625 [02:34<00:00,  4.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01 | train_loss=6.2215 | dev_ROUGE-L(F1)=0.0603\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.0603)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 625/625 [02:33<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 02 | train_loss=5.7126 | dev_ROUGE-L(F1)=0.1053\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.1053)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 625/625 [02:33<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 03 | train_loss=5.3568 | dev_ROUGE-L(F1)=0.1494\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.1494)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 625/625 [02:33<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 04 | train_loss=5.0525 | dev_ROUGE-L(F1)=0.1778\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.1778)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 625/625 [02:33<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 05 | train_loss=4.7949 | dev_ROUGE-L(F1)=0.1939\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.1939)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 625/625 [02:34<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 06 | train_loss=4.5758 | dev_ROUGE-L(F1)=0.2108\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2108)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 625/625 [02:33<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 07 | train_loss=4.3751 | dev_ROUGE-L(F1)=0.2142\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2142)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 625/625 [02:35<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 08 | train_loss=4.2027 | dev_ROUGE-L(F1)=0.2178\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2178)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 625/625 [02:34<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 09 | train_loss=4.0471 | dev_ROUGE-L(F1)=0.2382\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2382)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 625/625 [02:36<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 | train_loss=3.9050 | dev_ROUGE-L(F1)=0.2426\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2426)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 625/625 [02:33<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 | train_loss=3.7753 | dev_ROUGE-L(F1)=0.2535\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2535)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 625/625 [02:33<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 | train_loss=3.6562 | dev_ROUGE-L(F1)=0.2545\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2545)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 625/625 [02:32<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 | train_loss=3.5456 | dev_ROUGE-L(F1)=0.2601\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2601)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 625/625 [02:32<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 | train_loss=3.4434 | dev_ROUGE-L(F1)=0.2650\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2650)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 625/625 [02:32<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 | train_loss=3.3502 | dev_ROUGE-L(F1)=0.2744\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2744)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 625/625 [02:33<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 | train_loss=3.2638 | dev_ROUGE-L(F1)=0.2731\n",
            "[EARLY STOP] No improvement 1/5 (best=0.2744 at epoch 15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 625/625 [02:33<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 | train_loss=3.1776 | dev_ROUGE-L(F1)=0.2750\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2750)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 625/625 [02:32<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 | train_loss=3.0969 | dev_ROUGE-L(F1)=0.2763\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2763)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 625/625 [02:34<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 | train_loss=3.0203 | dev_ROUGE-L(F1)=0.2821\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2821)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 625/625 [02:34<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 | train_loss=2.9473 | dev_ROUGE-L(F1)=0.2821\n",
            "[EARLY STOP] No improvement 1/5 (best=0.2821 at epoch 19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 625/625 [02:33<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21 | train_loss=2.8772 | dev_ROUGE-L(F1)=0.2865\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2865)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 625/625 [02:34<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22 | train_loss=2.8141 | dev_ROUGE-L(F1)=0.2894\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2894)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 625/625 [02:34<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23 | train_loss=2.7489 | dev_ROUGE-L(F1)=0.2896\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2896)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 625/625 [02:32<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24 | train_loss=2.6878 | dev_ROUGE-L(F1)=0.2901\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2901)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|██████████| 625/625 [02:32<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25 | train_loss=2.6326 | dev_ROUGE-L(F1)=0.2961\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2961)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|██████████| 625/625 [02:34<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26 | train_loss=2.5760 | dev_ROUGE-L(F1)=0.2945\n",
            "[EARLY STOP] No improvement 1/5 (best=0.2961 at epoch 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|██████████| 625/625 [02:33<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27 | train_loss=2.5226 | dev_ROUGE-L(F1)=0.2937\n",
            "[EARLY STOP] No improvement 2/5 (best=0.2961 at epoch 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|██████████| 625/625 [02:32<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28 | train_loss=2.4699 | dev_ROUGE-L(F1)=0.2954\n",
            "[EARLY STOP] No improvement 3/5 (best=0.2961 at epoch 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|██████████| 625/625 [02:33<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29 | train_loss=2.4195 | dev_ROUGE-L(F1)=0.2969\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2969)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|██████████| 625/625 [02:34<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30 | train_loss=2.3732 | dev_ROUGE-L(F1)=0.2988\n",
            "[SAVE] Best -> best_lstm3_bahdanau_envi.pt (dev_ROUGE-L=0.2988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = torch.load(BEST_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "test_rouge = evaluate_rouge_l_attn(model, test_loader, tgt_vocab)\n",
        "print(\"TEST ROUGE-L(F1) =\", test_rouge)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkeNL7njigYG",
        "outputId": "d8ca6a9e-84ac-4aeb-e338-7f5d9b74cfc9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST ROUGE-L(F1) = 0.31492271826802154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for i in range(min(5, len(test_ds))):\n",
        "    en, vi = test_ds[i]\n",
        "    src_ids = torch.tensor([src_vocab.encode(basic_tokenize(en)[:MAX_LEN_SRC])], dtype=torch.long).to(DEVICE)\n",
        "    pred = greedy_decode_attn(model, src_ids, tgt_vocab, max_len=MAX_LEN_TGT)[0].tolist()\n",
        "    pred_text = \" \".join(tgt_vocab.decode(pred[1:]))\n",
        "\n",
        "    print(\"\\nEN:\", en)\n",
        "    print(\"GT:\", vi)\n",
        "    print(\"PR:\", pred_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx87Zv5miug5",
        "outputId": "0430b88a-fb5b-4fe8-da7b-ade5bd3c7be6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EN: Brother Albert Barnett and his wife , Sister Susan Barnett , from the West Congregation in Tuscaloosa , Alabama\n",
            "GT: Anh Albert Barnett và chị Susan Barnett , thuộc hội thánh West ở Tuscaloosa , Alabama\n",
            "PR: Người đàn ông và Domitia khóc , Tom nhân viên , anh ấy , nhà thờ trường học ở trường , trong những người không gian .\n",
            "\n",
            "EN: Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12 , 2020 .\n",
            "GT: Ngày 11 và 12-1-2020 , những cơn bão lớn đã quét qua và phá huỷ nhiều vùng ở miền nam và miền trung Hoa Kỳ .\n",
            "PR: Một người mẹ đã trải qua các thành phố trong vòng và các nước Mỹ và Trung Quốc đã mất một tỷ lệ và tội phạm .\n",
            "\n",
            "EN: Two days of heavy rain , high winds , and numerous tornadoes caused major damage across multiple states .\n",
            "GT: Những trận mưa to và gió lớn trong suốt hai ngày cùng với nhiều cơn lốc xoáy đã gây thiệt hại nặng nề cho nhiều bang .\n",
            "PR: Hai năm trước , những người ủng hộ , và Angelina thẳng thắn của các người khác đều bị bạo lực .\n",
            "\n",
            "EN: Sadly , Brother Albert Barnett and his wife , Sister Susan Barnett , 85 and 75 years old respectively , were killed when a tornado struck their mobile home .\n",
            "GT: Đáng buồn là anh Albert Barnett 85 tuổi , và vợ anh là chị Susan Barnett 75 tuổi đã thiệt mạng do một cơn lốc xoáy quét qua nhà họ .\n",
            "PR: Người mẹ không nói , và vợ vợ chồng chồng , anh trai , những người trẻ tuổi , tuổi thọ , anh ta đã viết về một cái nhìn vào nhà của họ .\n",
            "\n",
            "EN: The United States branch also reports that at least four of our brothers ' homes sustained minor damage , along with two Kingdom Halls .\n",
            "GT: Chi nhánh Hoa Kỳ cũng cho biết có ít nhất bốn căn nhà của anh em chúng tôi và hai Phòng Nước Trời bị hư hại nhẹ .\n",
            "PR: Nhà trợ của chính phủ được nhận được bởi một người đàn ông này cho tôi \" Với những bức tường của các nhà vua , với hai con số .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bài 3: Xây dựng kiến trúc Encoder-Decoder gồm 3 lớp LSTM cho module encoder và 3 lớp LSTM cho module decoder, với hidden size là 256, cho bài toán dịch máy từ tiếng Anh sang tiếng Việt. Module decoder được trang bị kỹ thuật attention theo mô tả của nghiên cứu \"[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)\". Huấn luyện mô hình này trên bộ dữ liệu PhoMT sử dụng Adam làm phương thức tối ưu tham số. Đánh giá độ hiệu quả của mô hình sử dụn độ đo ROUGE-L."
      ],
      "metadata": {
        "id": "I3UOGKQ91_hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "EMBED_DIM = 256\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.2\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size: int, pad_id: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=pad_id)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=EMBED_DIM,\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            dropout=DROPOUT if NUM_LAYERS > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "    def forward(self, src_ids):\n",
        "        # src_ids: [B,S]\n",
        "        x = self.emb(src_ids)                 # [B,S,E]\n",
        "        enc_outputs, (h, c) = self.lstm(x)    # enc_outputs: [B,S,H]\n",
        "        return enc_outputs, (h, c)\n",
        "\n",
        "\n",
        "#Class attention giống bạn đưa + thêm mask PAD\n",
        "class LuongAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Luong 'general' attention:\n",
        "      score(h_t, h_i) = h_t^T W h_i\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs, src_mask=None):\n",
        "        \"\"\"\n",
        "        decoder_hidden: [B,H]\n",
        "        encoder_outputs: [B,S,H]\n",
        "        src_mask: [B,S] True for valid tokens (non-PAD)\n",
        "        \"\"\"\n",
        "        query = decoder_hidden.unsqueeze(1)         # [B,1,H]\n",
        "        keys = self.W(encoder_outputs)              # [B,S,H]\n",
        "        scores = torch.bmm(query, keys.transpose(1,2)).squeeze(1)  # [B,S]\n",
        "\n",
        "        if src_mask is not None:\n",
        "            scores = scores.masked_fill(~src_mask, -1e9)\n",
        "\n",
        "        attn_w = torch.softmax(scores, dim=1)       # [B,S]\n",
        "        context = torch.bmm(attn_w.unsqueeze(1), encoder_outputs).squeeze(1)  # [B,H]\n",
        "        return context, attn_w\n",
        "\n",
        "\n",
        "class LuongDecoderInputFeeding(nn.Module):\n",
        "    \"\"\"\n",
        "    - input_t = [emb(y_{t-1}); tilde_{t-1}]\n",
        "    - attention dùng h_t (output LSTM) để lấy context\n",
        "    - tilde_t = tanh(Wc [context; h_t])\n",
        "    - dự đoán token từ tilde_t\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int, pad_id: int):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, EMBED_DIM, padding_idx=pad_id)\n",
        "        self.attn = LuongAttention(HIDDEN_SIZE)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=EMBED_DIM + HIDDEN_SIZE,   # input-feeding\n",
        "            hidden_size=HIDDEN_SIZE,\n",
        "            num_layers=NUM_LAYERS,\n",
        "            dropout=DROPOUT if NUM_LAYERS > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.Wc = nn.Linear(HIDDEN_SIZE + HIDDEN_SIZE, HIDDEN_SIZE)  # attentional vector\n",
        "        self.proj = nn.Linear(HIDDEN_SIZE, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_in_ids, enc_outputs, init_state, src_mask=None):\n",
        "        \"\"\"\n",
        "        tgt_in_ids: [B,T] (BOS + y)\n",
        "        returns logits: [B,T,V]\n",
        "        \"\"\"\n",
        "        h, c = init_state\n",
        "        B, T = tgt_in_ids.size()\n",
        "\n",
        "        tilde_prev = torch.zeros(B, HIDDEN_SIZE, device=tgt_in_ids.device)  # \\tilde{h}_0 = 0\n",
        "        logits_all = []\n",
        "\n",
        "        for t in range(T):\n",
        "            y_t = tgt_in_ids[:, t]             # [B]\n",
        "            emb_t = self.emb(y_t)              # [B,E]\n",
        "\n",
        "            lstm_in = torch.cat([emb_t, tilde_prev], dim=-1).unsqueeze(1)  # [B,1,E+H]\n",
        "            out_t, (h, c) = self.lstm(lstm_in, (h, c))\n",
        "            h_t = out_t.squeeze(1)             # [B,H]\n",
        "\n",
        "            context, _ = self.attn(h_t, enc_outputs, src_mask=src_mask)    # [B,H]\n",
        "            tilde_t = torch.tanh(self.Wc(torch.cat([context, h_t], dim=-1))) # [B,H]\n",
        "\n",
        "            logits_t = self.proj(tilde_t)      # [B,V]\n",
        "            logits_all.append(logits_t.unsqueeze(1))\n",
        "\n",
        "            tilde_prev = tilde_t\n",
        "\n",
        "        return torch.cat(logits_all, dim=1)    # [B,T,V]\n",
        "\n",
        "\n",
        "class Seq2SeqLuong(nn.Module):\n",
        "    def __init__(self, enc: Encoder, dec: LuongDecoderInputFeeding, pad_id_src: int, pad_id_tgt: int):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "        self.pad_id_src = pad_id_src\n",
        "        self.loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id_tgt)\n",
        "\n",
        "    def make_src_mask(self, src_ids):\n",
        "        return (src_ids != self.pad_id_src)  # [B,S] True for non-pad\n",
        "\n",
        "    def forward(self, src_ids, tgt_in, tgt_out):\n",
        "        enc_outputs, (h, c) = self.enc(src_ids)\n",
        "        src_mask = self.make_src_mask(src_ids)\n",
        "        logits = self.dec(tgt_in, enc_outputs, (h, c), src_mask=src_mask)\n",
        "        loss = self.loss_fn(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        return loss\n",
        "\n",
        "\n",
        "# build model\n",
        "enc = Encoder(len(src_vocab), src_vocab.pad_id)\n",
        "dec = LuongDecoderInputFeeding(len(tgt_vocab), tgt_vocab.pad_id)\n",
        "model = Seq2SeqLuong(enc, dec, pad_id_src=src_vocab.pad_id, pad_id_tgt=tgt_vocab.pad_id).to(DEVICE)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSFHEh0t2qJr",
        "outputId": "308ea41f-59a3-4240-e166-ea42840bf68d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2SeqLuong(\n",
            "  (enc): Encoder(\n",
            "    (emb): Embedding(19065, 256, padding_idx=0)\n",
            "    (lstm): LSTM(256, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  )\n",
            "  (dec): LuongDecoderInputFeeding(\n",
            "    (emb): Embedding(8297, 256, padding_idx=0)\n",
            "    (attn): LuongAttention(\n",
            "      (W): Linear(in_features=256, out_features=256, bias=False)\n",
            "    )\n",
            "    (lstm): LSTM(512, 256, num_layers=3, batch_first=True, dropout=0.2)\n",
            "    (Wc): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (proj): Linear(in_features=256, out_features=8297, bias=True)\n",
            "  )\n",
            "  (loss_fn): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))\n",
        "loss = model(batch.src.to(DEVICE), batch.tgt_in.to(DEVICE), batch.tgt_out.to(DEVICE))\n",
        "print(\"sanity loss:\", float(loss.item()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmkia5UB3miu",
        "outputId": "7f8ad0c3-c941-458a-aab4-b5e470fc6b2f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sanity loss: 9.026552200317383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lcs_length(a: List[str], b: List[str]) -> int:\n",
        "    n, m = len(a), len(b)\n",
        "    dp = [0]*(m+1)\n",
        "    for i in range(1, n+1):\n",
        "        prev = 0\n",
        "        for j in range(1, m+1):\n",
        "            tmp = dp[j]\n",
        "            if a[i-1] == b[j-1]:\n",
        "                dp[j] = prev + 1\n",
        "            else:\n",
        "                dp[j] = max(dp[j], dp[j-1])\n",
        "            prev = tmp\n",
        "    return dp[m]\n",
        "\n",
        "def rouge_l_f1(pred: List[str], ref: List[str]) -> float:\n",
        "    if not pred or not ref:\n",
        "        return 0.0\n",
        "    lcs = lcs_length(pred, ref)\n",
        "    p = lcs / max(1, len(pred))\n",
        "    r = lcs / max(1, len(ref))\n",
        "    return 0.0 if (p+r)==0 else (2*p*r)/(p+r)\n",
        "\n",
        "@torch.no_grad()\n",
        "def greedy_decode_luong(model: Seq2SeqLuong, src_ids: torch.Tensor, tgt_vocab: Vocab, max_len=190):\n",
        "    model.eval()\n",
        "    enc_outputs, (h, c) = model.enc(src_ids)\n",
        "    src_mask = (src_ids != model.pad_id_src)\n",
        "\n",
        "    B = src_ids.size(0)\n",
        "    ys = torch.full((B,1), tgt_vocab.bos_id, dtype=torch.long, device=src_ids.device)\n",
        "\n",
        "    tilde_prev = torch.zeros(B, HIDDEN_SIZE, device=src_ids.device)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        y_last = ys[:, -1]                    # [B]\n",
        "        emb_t = model.dec.emb(y_last)         # [B,E]\n",
        "        lstm_in = torch.cat([emb_t, tilde_prev], dim=-1).unsqueeze(1)  # [B,1,E+H]\n",
        "\n",
        "        out_t, (h, c) = model.dec.lstm(lstm_in, (h, c))\n",
        "        h_t = out_t.squeeze(1)                # [B,H]\n",
        "\n",
        "        context, _ = model.dec.attn(h_t, enc_outputs, src_mask=src_mask)\n",
        "        tilde_t = torch.tanh(model.dec.Wc(torch.cat([context, h_t], dim=-1)))\n",
        "        logits = model.dec.proj(tilde_t)      # [B,V]\n",
        "\n",
        "        next_tok = logits.argmax(-1, keepdim=True)\n",
        "        ys = torch.cat([ys, next_tok], dim=1)\n",
        "\n",
        "        tilde_prev = tilde_t\n",
        "\n",
        "        if (next_tok.squeeze(1) == tgt_vocab.eos_id).all():\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_rouge_l_luong(model, loader, tgt_vocab: Vocab):\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    for batch in loader:\n",
        "        src = batch.src.to(DEVICE)\n",
        "        pred_ids = greedy_decode_luong(model, src, tgt_vocab, max_len=MAX_LEN_TGT)[:, 1:].tolist()\n",
        "        for i in range(len(pred_ids)):\n",
        "            pred_toks = tgt_vocab.decode(pred_ids[i])\n",
        "            ref_toks  = tgt_vocab.decode(batch.tgt_out[i].tolist())\n",
        "            scores.append(rouge_l_f1(pred_toks, ref_toks))\n",
        "    return sum(scores)/max(1,len(scores))\n"
      ],
      "metadata": {
        "id": "_yZGQpCq3psZ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 3e-4\n",
        "EPOCHS = 30\n",
        "GRAD_CLIP = 1.0\n",
        "PATIENCE = 5\n",
        "MIN_DELTA = 1e-4\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "best_dev = -1.0\n",
        "best_epoch = 0\n",
        "bad_epochs = 0\n",
        "BEST_PATH = \"best_lstm3_luong_envi.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
        "        src = batch.src.to(DEVICE)\n",
        "        tgt_in = batch.tgt_in.to(DEVICE)\n",
        "        tgt_out = batch.tgt_out.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(src, tgt_in, tgt_out)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "    train_loss = total_loss / max(1, len(train_loader))\n",
        "    dev_rouge = evaluate_rouge_l_luong(model, dev_loader, tgt_vocab)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:02d} | train_loss={train_loss:.4f} | dev_ROUGE-L(F1)={dev_rouge:.4f}\")\n",
        "\n",
        "    if dev_rouge > best_dev + MIN_DELTA:\n",
        "        best_dev = dev_rouge\n",
        "        best_epoch = epoch\n",
        "        bad_epochs = 0\n",
        "        torch.save({\"model_state\": model.state_dict()}, BEST_PATH)\n",
        "        print(f\"[SAVE] Best -> {BEST_PATH} (dev_ROUGE-L={best_dev:.4f})\")\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        print(f\"[EARLY STOP] No improvement {bad_epochs}/{PATIENCE} (best={best_dev:.4f} at epoch {best_epoch})\")\n",
        "        if bad_epochs >= PATIENCE:\n",
        "            print(f\"[EARLY STOP] Stop. Best dev ROUGE-L={best_dev:.4f} at epoch {best_epoch}\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peTLmX_B3rmY",
        "outputId": "5d11eaa8-88c8-4f6b-d6cd-2712184ab37a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|██████████| 625/625 [02:27<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01 | train_loss=6.3189 | dev_ROUGE-L(F1)=0.0750\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.0750)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 625/625 [02:28<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 02 | train_loss=5.9989 | dev_ROUGE-L(F1)=0.1125\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.1125)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 625/625 [02:30<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 03 | train_loss=5.6581 | dev_ROUGE-L(F1)=0.1185\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.1185)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 625/625 [02:29<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 04 | train_loss=5.3050 | dev_ROUGE-L(F1)=0.1477\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.1477)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 625/625 [02:28<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 05 | train_loss=5.0263 | dev_ROUGE-L(F1)=0.1973\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.1973)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 625/625 [02:27<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 06 | train_loss=4.7866 | dev_ROUGE-L(F1)=0.2083\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2083)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 625/625 [02:27<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 07 | train_loss=4.5783 | dev_ROUGE-L(F1)=0.2112\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2112)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 625/625 [02:27<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 08 | train_loss=4.3974 | dev_ROUGE-L(F1)=0.2239\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2239)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 625/625 [02:28<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 09 | train_loss=4.2381 | dev_ROUGE-L(F1)=0.2335\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2335)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 625/625 [02:28<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10 | train_loss=4.0953 | dev_ROUGE-L(F1)=0.2442\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2442)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 625/625 [02:28<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11 | train_loss=3.9665 | dev_ROUGE-L(F1)=0.2485\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2485)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 625/625 [02:30<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12 | train_loss=3.8470 | dev_ROUGE-L(F1)=0.2558\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2558)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 625/625 [02:28<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13 | train_loss=3.7381 | dev_ROUGE-L(F1)=0.2629\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2629)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 625/625 [02:27<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14 | train_loss=3.6384 | dev_ROUGE-L(F1)=0.2695\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2695)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 625/625 [02:29<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15 | train_loss=3.5456 | dev_ROUGE-L(F1)=0.2742\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2742)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 625/625 [02:28<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16 | train_loss=3.4600 | dev_ROUGE-L(F1)=0.2813\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2813)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 625/625 [02:27<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17 | train_loss=3.3791 | dev_ROUGE-L(F1)=0.2800\n",
            "[EARLY STOP] No improvement 1/5 (best=0.2813 at epoch 16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 625/625 [02:25<00:00,  4.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18 | train_loss=3.3034 | dev_ROUGE-L(F1)=0.2885\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2885)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 625/625 [02:27<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19 | train_loss=3.2314 | dev_ROUGE-L(F1)=0.2893\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2893)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 625/625 [02:26<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20 | train_loss=3.1630 | dev_ROUGE-L(F1)=0.2918\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2918)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 625/625 [02:26<00:00,  4.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21 | train_loss=3.0984 | dev_ROUGE-L(F1)=0.2969\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2969)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 625/625 [02:27<00:00,  4.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22 | train_loss=3.0370 | dev_ROUGE-L(F1)=0.2976\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.2976)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 625/625 [02:25<00:00,  4.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23 | train_loss=2.9778 | dev_ROUGE-L(F1)=0.3009\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.3009)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 625/625 [02:27<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24 | train_loss=2.9188 | dev_ROUGE-L(F1)=0.3028\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.3028)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|██████████| 625/625 [02:27<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25 | train_loss=2.8646 | dev_ROUGE-L(F1)=0.3066\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.3066)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|██████████| 625/625 [02:26<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26 | train_loss=2.8094 | dev_ROUGE-L(F1)=0.3084\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.3084)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|██████████| 625/625 [02:26<00:00,  4.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27 | train_loss=2.7589 | dev_ROUGE-L(F1)=0.3106\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.3106)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|██████████| 625/625 [02:30<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28 | train_loss=2.7093 | dev_ROUGE-L(F1)=0.3144\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.3144)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|██████████| 625/625 [02:29<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29 | train_loss=2.6597 | dev_ROUGE-L(F1)=0.3117\n",
            "[EARLY STOP] No improvement 1/5 (best=0.3144 at epoch 28)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|██████████| 625/625 [02:27<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30 | train_loss=2.6157 | dev_ROUGE-L(F1)=0.3162\n",
            "[SAVE] Best -> best_lstm3_luong_envi.pt (dev_ROUGE-L=0.3162)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = torch.load(BEST_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "test_rouge = evaluate_rouge_l_luong(model, test_loader, tgt_vocab)\n",
        "print(\"TEST ROUGE-L(F1) =\", test_rouge)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q44cBu23vsd",
        "outputId": "2e66033c-cf75-47e6-b1e4-27027bcf6d04"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST ROUGE-L(F1) = 0.33071683254656103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "for i in range(min(5, len(test_ds))):\n",
        "    en, vi = test_ds[i]\n",
        "    src_ids = torch.tensor([src_vocab.encode(basic_tokenize(en)[:MAX_LEN_SRC])], dtype=torch.long).to(DEVICE)\n",
        "    pred = greedy_decode_luong(model, src_ids, tgt_vocab, max_len=MAX_LEN_TGT)[0].tolist()\n",
        "    pred_text = \" \".join(tgt_vocab.decode(pred[1:]))\n",
        "\n",
        "    print(\"\\nEN:\", en)\n",
        "    print(\"GT:\", vi)\n",
        "    print(\"PR:\", pred_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEzIOaFq3xJ2",
        "outputId": "cec73004-673f-4ab7-c3de-fda2c3191c2b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EN: Brother Albert Barnett and his wife , Sister Susan Barnett , from the West Congregation in Tuscaloosa , Alabama\n",
            "GT: Anh Albert Barnett và chị Susan Barnett , thuộc hội thánh West ở Tuscaloosa , Alabama\n",
            "PR: Những người phản ứng và vợ tôi , đã cho phép một người ủng hộ , từ những nhà lãnh đạo ở New York ,\n",
            "\n",
            "EN: Severe storms ripped through parts of the southern and midwestern United States on January 11 and 12 , 2020 .\n",
            "GT: Ngày 11 và 12-1-2020 , những cơn bão lớn đã quét qua và phá huỷ nhiều vùng ở miền nam và miền trung Hoa Kỳ .\n",
            "PR: Nó cho phép nó vượt qua những phần ở Kenya và các nhà báo Mỹ đã giảm tuổi da , và 12 ngày , người Nhật .\n",
            "\n",
            "EN: Two days of heavy rain , high winds , and numerous tornadoes caused major damage across multiple states .\n",
            "GT: Những trận mưa to và gió lớn trong suốt hai ngày cùng với nhiều cơn lốc xoáy đã gây thiệt hại nặng nề cho nhiều bang .\n",
            "PR: Một ngày trong thời gian của tôi , những nhà thờ Châu Phi , và có vẻ phản ứng có vẻ phản ứng trên khắp khắp khắp khắp khắp khắp khắp khắp đất đất .\n",
            "\n",
            "EN: Sadly , Brother Albert Barnett and his wife , Sister Susan Barnett , 85 and 75 years old respectively , were killed when a tornado struck their mobile home .\n",
            "GT: Đáng buồn là anh Albert Barnett 85 tuổi , và vợ anh là chị Susan Barnett 75 tuổi đã thiệt mạng do một cơn lốc xoáy quét qua nhà họ .\n",
            "PR: Một người , những người bị rối loạn và vợ chồng , đã yêu cầu một con ngựa , Missouri - - 20 năm , có thể nhìn thấy một tờ báo cho các nhà hàng tuỷ đời .\n",
            "\n",
            "EN: The United States branch also reports that at least four of our brothers ' homes sustained minor damage , along with two Kingdom Halls .\n",
            "GT: Chi nhánh Hoa Kỳ cũng cho biết có ít nhất bốn căn nhà của anh em chúng tôi và hai Phòng Nước Trời bị hư hại nhẹ .\n",
            "PR: Mỹ có thể học được rằng có hai ví dụ trong những ví dụ của ông ấy đã trải qua những cảm xúc của bạn , trong hai nhóm số các bài thuyết trình .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lưu lại bộ vocab và token"
      ],
      "metadata": {
        "id": "1oQZq7NMBBd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_PATH = \"phomt_vocab_shared.pt\"\n",
        "\n",
        "torch.save({\n",
        "    \"src_itos\": src_vocab.itos,\n",
        "    \"tgt_itos\": tgt_vocab.itos,\n",
        "    \"src_special\": {\"pad\": src_vocab.pad_id, \"unk\": src_vocab.unk_id, \"bos\": src_vocab.bos_id, \"eos\": src_vocab.eos_id},\n",
        "    \"tgt_special\": {\"pad\": tgt_vocab.pad_id, \"unk\": tgt_vocab.unk_id, \"bos\": tgt_vocab.bos_id, \"eos\": tgt_vocab.eos_id},\n",
        "    \"max_len_src\": MAX_LEN_SRC,\n",
        "    \"max_len_tgt\": MAX_LEN_TGT,\n",
        "}, VOCAB_PATH)\n",
        "\n",
        "print(\"Saved shared vocab ->\", VOCAB_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El-eAGxqAqtL",
        "outputId": "6f42c289-3a30-40dd-c39e-702b13a85a97"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved shared vocab -> phomt_vocab_shared.pt\n"
          ]
        }
      ]
    }
  ]
}